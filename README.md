# Server Code for ML Inference Analysis 

Working through deployment of NVIDIA Triton Server using grpc and Object Detection model. 

Goals:
*   FastAPI REST API in container
*   Using GRPC API
*   Triton + GRPC
    *   Triton working with multiheaded algorithms
    *   Server code in Python
    *   Server code in Cpp
    *   Server code in Rust
*   Triton + custom GRPC + Cpp API 
*   Different compiled models:
    *   Non compiled
    *   TensorRT
    *   Torchscript
    
