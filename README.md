# Server Code for ML Inference Analysis 

Working through deployment of NVIDIA Triton Server using grpc and Object Detection model. 

Goals:
*   Flask REST API in container
*   FastAPI REST API in container
*   Using GRPC API
*   Triton + GRPC
    *   Triton working with multiheaded algorithms
    *   Server code in Python
    *   Server code in Cpp
    *   Server code in Rust
*   Triton + custom GRPC + Cpp API 
*   Different compiled models:
    *   Non compiled
    *   TensorRT
    *   TorchScript
*   CPU/GPU
*   Image/Video
*   Client Server
*   Data Logging and Analysis
    * What's important?
    
    
    
